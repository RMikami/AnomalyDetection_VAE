{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,Lambda,BatchNormalization,Activation,Flatten,Reshape\n",
    "from keras.layers.convolutional import Conv2D,Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives,metrics\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors \n",
    "from sklearn import metrics\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像の切り出し\n",
    "元の画像から一部を切り取って学習データを作成します。\n",
    "ここでは、256×256サイズから11×11サイズを切り取り、1万枚を学習データとして用意しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_img(x,number,height=11,width=11):\n",
    "    x_out=[]\n",
    "    for i in range(number):\n",
    "        shape_0=np.random.randint(0,x.shape[0])\n",
    "        shape_1=np.random.randint(0,x.shape[1]-height)\n",
    "        shape_2=np.random.randint(0,x.shape[2]-width)\n",
    "        temp=x[shape_0,shape_1:shape_1+height,shape_2:shape_2+width,0]\n",
    "        x_out.append(temp.reshape(11,11,1))\n",
    "    x_out=np.array(x_out)\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの作成\n",
    "学習用データ：391枚\n",
    "正常のテスト用データ：40枚\n",
    "異常(crack,cut,hole,print)のテスト用データ：62枚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_1=os.listdir(\".../anomaly_detection/hazelnut/train/good\")\n",
    "path_2=os.listdir(\".../anomaly_detection/hazelnut/test/good\")\n",
    "path_3=os.listdir(\".../anomaly_detection/hazelnut/test/bad\")\n",
    "\n",
    "file_1=\".../anomaly_detection/hazelnut/train/good/\"\n",
    "file_2=\".../anomaly_detection/hazelnut/test/good/\"\n",
    "file_3=\".../anomaly_detection/hazelnut/test/bad/\"\n",
    "\n",
    "x_train_normal=[]\n",
    "x_test_normal=[]\n",
    "x_test_anomaly=[]\n",
    "\n",
    "#学習用データの抽出\n",
    "for i in path_1:\n",
    "    img_1=image.load_img(file_1+i,grayscale=True,target_size=(256,256))\n",
    "    x_train=image.img_to_array(img_1)\n",
    "    x_train=x_train.astype(\"float32\")/255\n",
    "    x_train_normal.append(x_train)\n",
    "x_train_normal=np.array(x_train_normal)\n",
    "x_train_normal=cut_img(x_train_normal,10000)\n",
    "#print(x_train_normal.shape)\n",
    "\n",
    "#正常のテスト用データの抽出\n",
    "for i in path_2:\n",
    "    img_2=image.load_img(file_2+i,grayscale=True,target_size=(256,256))\n",
    "    x_test_good=image.img_to_array(img_2)\n",
    "    x_test_good=x_test_good.astype(\"float32\")/255\n",
    "    x_test_normal.append(x_test_good)\n",
    "x_test_normal=np.array(x_test_normal)\n",
    "test_normal=x_test_normal[np.random.randint(len(x_test_normal))]\n",
    "test_normal=test_normal.reshape(1,256,256,1)\n",
    "#print(test_normal.shape)\n",
    "\n",
    "#異常のテスト用データの抽出\n",
    "for i in path_3:\n",
    "    img_3=image.load_img(file_3+i,grayscale=True,target_size=(256,256))\n",
    "    x_test_bad=image.img_to_array(img_3)\n",
    "    x_test_bad=x_test_bad.astype(\"float32\")/255\n",
    "    x_test_anomaly.append(x_test_bad)\n",
    "x_test_anomaly=np.array(x_test_anomaly)\n",
    "test_anomaly=x_test_anomaly[np.random.randint(len(x_test_anomaly))]\n",
    "test_anomaly=test_anomaly.reshape(1,256,256,1)\n",
    "#print(test_anomaly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "input_shape=(11,11,1)\n",
    "latent_dim=2\n",
    "Nc=16\n",
    "epochs=30\n",
    "epsilon_std=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAEモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#サンプリング関数の作成\n",
    "def sampling(args):\n",
    "    z_mean,z_log_var=args\n",
    "    epsilon=K.random_normal(shape=(K.shape(z_mean)[0],latent_dim),mean=0,stddev=epsilon_std)\n",
    "    return z_mean+K.exp(0.5*z_log_var)*epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#エンコーダの作成\n",
    "inputs=Input(shape=input_shape,name=\"encoder_input\")\n",
    "x=Conv2D(Nc,kernel_size=2,strides=2)(inputs)\n",
    "x=BatchNormalization()(x)\n",
    "x=Activation(\"relu\")(x)\n",
    "x=Conv2D(Nc*2,kernel_size=2,strides=2)(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Activation(\"relu\")(x)\n",
    "x=Flatten()(x)\n",
    "\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    " \n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#デコーダの作成\n",
    "latent_inputs=Input(shape=(latent_dim,),name=\"z_sampling\")\n",
    "x=Dense(2*2)(latent_inputs)\n",
    "x=BatchNormalization()(x)\n",
    "x=Activation(\"relu\")(x)\n",
    "x=Reshape((2,2,1))(x)\n",
    "x=Conv2DTranspose(2*Nc,kernel_size=2,strides=2)(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Activation(\"relu\")(x)\n",
    "x=Conv2DTranspose(Nc,kernel_size=2,strides=2)(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=Activation(\"relu\")(x)\n",
    "\n",
    "x1=Conv2DTranspose(1,kernel_size=4)(x)\n",
    "x1=BatchNormalization()(x1)\n",
    "out1=Activation(\"sigmoid\")(x1)\n",
    "\n",
    "x2=Conv2DTranspose(1,kernel_size=4)(x)\n",
    "x2=BatchNormalization()(x2)\n",
    "out2=Activation(\"sigmoid\")(x2)\n",
    "\n",
    "decoder=Model(latent_inputs,[out1,out2],name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルをまとめる\n",
    "outputs_mu,outputs_sigma2=decoder(encoder(inputs)[2])\n",
    "vae=Model(inputs,[outputs_mu,outputs_sigma2],name=\"vae_mlp\")\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数の定義\n",
    "m_vae_loss=(K.flatten(outputs_mu)-K.flatten(inputs))**2/K.flatten(outputs_sigma2)\n",
    "m_vae_loss=0.5*K.sum(m_vae_loss)\n",
    "\n",
    "a_vae_loss=K.log(2*3.14*K.flatten(outputs_sigma2)+1e-7)\n",
    "a_vae_loss=0.5*K.sum(a_vae_loss)\n",
    "\n",
    "kl_loss=1+z_log_var-K.square(z_mean)-K.exp(z_log_var)\n",
    "kl_loss=-0.5*K.sum(kl_loss,axis=-1)\n",
    "\n",
    "vae_loss=K.mean(kl_loss+a_vae_loss+m_vae_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer=\"adam\")\n",
    "\n",
    "vae.fit(x_train_normal,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ヒートマップの作成\n",
    "256×256サイズの画像に11×11サイズの小窓を上下左右に走らせ、異常スコアを累積させていく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ヒートマップの計算\n",
    "def evaluate_img(model, x_normal, x_anomaly, name, height=11, width=11, move=5):  \n",
    "    img_normal = np.zeros((x_normal.shape))\n",
    "    img_anomaly = np.zeros((x_normal.shape))\n",
    "    \n",
    "    for i in range(int((x_normal.shape[1]-height)/move)+1):\n",
    "        for j in range(int((x_normal.shape[2]-width)/move)+1):\n",
    "            x_sub_normal = x_normal[0, i*move:i*move+height, j*move:j*move+width, 0]\n",
    "            x_sub_anomaly = x_anomaly[0, i*move:i*move+height, j*move:j*move+width, 0]\n",
    "            x_sub_normal = x_sub_normal.reshape(1, height, width, 1)\n",
    "            x_sub_anomaly = x_sub_anomaly.reshape(1, height, width, 1)\n",
    "            \n",
    "            #従来手法\n",
    "            if name == \"old_\":\n",
    "                normal_score = model.evaluate(x_sub_normal, batch_size=1, verbose=0)\n",
    "                img_normal[0, i*move:i*move+height, j*move:j*move+width, 0] +=  normal_score\n",
    "                \n",
    "                anomaly_score = model.evaluate(x_sub_anomaly, batch_size=1, verbose=0)\n",
    "                img_anomaly[0, i*move:i*move+height, j*move:j*move+width, 0] +=  anomaly_score\n",
    "                \n",
    "            #提案手法\n",
    "            else:\n",
    "                mu,sigma=model.predict(x_sub_normal,batch_size=1,verbose=0)\n",
    "                loss=0\n",
    "                for k in range(height):\n",
    "                    for l in range(width):\n",
    "                        loss+=0.5*(mu[0,k,l,0]-x_sub_normal[0,k,l,0])**2/sigma[0,k,l,0]\n",
    "                img_normal[0,i*move:i*move+height,j*move:j*move+width,0]+=loss\n",
    "                \n",
    "                mu,sigma=model.predict(x_sub_anomaly,batch_size=1,verbose=0)\n",
    "                loss=0\n",
    "                for k in range(height):\n",
    "                    for l in range(width):\n",
    "                        loss+=0.5*(mu[0,k,l,0]-x_sub_anomaly[0,k,l,0])**2/sigma[0,k,l,0]\n",
    "                img_anomaly[0,i*move:i*move+height,j*move:j*move+width,0]+=loss\n",
    "    \n",
    "    save_img(x_normal,x_anomaly,img_normal,img_anomaly,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ヒートマップの描画\n",
    "def save_img(x_normal,x_anomaly,img_normal,img_anomaly,name):\n",
    "    path=\".../anomaly_detection/vae/hazelnut/images/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    #計算したヒートマップを1～10に正規化\n",
    "    img_max=np.max([img_normal,img_anomaly])\n",
    "    img_min=np.min([img_normal,img_anomaly])\n",
    "    img_normal=(img_normal-img_min)/(img_max-img_min)*9+1\n",
    "    img_anomaly=(img_anomaly-img_min)/(img_max-img_min)*9+1\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(x_normal[0,:,:,0],cmap=\"gray\")\n",
    "    plt.axis(\"off\") \n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(img_normal[0,:,:,0],cmap=\"Blues\",norm=colors.LogNorm())\n",
    "    plt.axis(\"off\")\n",
    "    plt.colorbar()\n",
    "    plt.clim(1,10)\n",
    "    plt.title(name+\"normal\")\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(x_anomaly[0,:,:,0],cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(img_anomaly[0,:,:,0],cmap=\"Blues\",norm=colors.LogNorm())\n",
    "    plt.axis(\"off\")\n",
    "    plt.colorbar()\n",
    "    plt.clim(1,10)\n",
    "    plt.title(name+\"anomaly\")\n",
    "    \n",
    "    plt.savefig(path+name+\".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "evaluate_img(vae,test_normal,test_anomaly,\"old_\")\n",
    "evaluate_img(vae,test_normal,test_anomaly,\"new_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最大異常スコアの算出\n",
    "テストデータ1枚1枚に11×11サイズの小窓を走らせ、算出されたスコアの中からその最大値をその画像の異常スコアとした。\n",
    "そして閾値を定め、閾値より異常スコアが高いものを「異常画像」、低いものを「正常画像」と判定しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_score(model,x,name,height=11,width=11,move=5):\n",
    "    score=[]\n",
    "    for k in range(len(x)):\n",
    "        max_score=-100000000000000\n",
    "        if k%10==0:\n",
    "            print(k)\n",
    "            \n",
    "        for i in range(int((x.shape[1]-height)/move)+1):\n",
    "            for j in range(int((x.shape[2]-width)/move)+1):\n",
    "                x_sub=x[k,i*move:i*move+height,j*move:j*move+width,0]\n",
    "                x_sub=x_sub.reshape(1,height,width,1)\n",
    "                \n",
    "                if name==\"old_\":\n",
    "                    temp_score=model.evaluate(x_sub,batch_size=1,verbose=0)\n",
    "                    if temp_score>max_score:\n",
    "                        max_score=temp_score\n",
    "                \n",
    "                else:\n",
    "                    mu,sigma=model.predict(x_sub,batch_size=1,verbose=0)\n",
    "                    loss=0\n",
    "                    for o in range(height):\n",
    "                        for l in range(width):\n",
    "                            loss+=0.5*(mu[0,o,l,0]-x_sub[0,o,l,0])**2/sigma[0,o,l,0]\n",
    "                    if loss>max_score:\n",
    "                        max_score=loss\n",
    "        \n",
    "        score.append(max_score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_normal=x_test_normal\n",
    "test_anomaly=x_test_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normal test data:\",len(test_normal))\n",
    "old_score_normal=result_score(vae,test_normal,\"old_\")\n",
    "#print(old_score_normal)\n",
    "\n",
    "print(\"anomaly test data:\",len(test_anomaly))\n",
    "old_score_anomaly=result_score(vae,test_anomaly,\"old_\")\n",
    "#print(old_score_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normal test data:\",len(test_normal))\n",
    "new_score_normal=result_score(vae,test_normal,\"new_\")\n",
    "#print(new_score_normal)\n",
    "\n",
    "print(\"anomaly test data:\",len(test_anomaly))\n",
    "new_score_anomaly=result_score(vae,test_anomaly,\"new_\")\n",
    "#print(new_score_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC曲線の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.zeros(len(test_normal)+len(test_anomaly))\n",
    "y_true[len(test_normal):]=1　#0:正常、1:異常\n",
    "old_score=np.array(old_score_normal)\n",
    "old_score=np.hstack((old_score,np.array(old_score_anomaly)))\n",
    "new_score=np.array(new_score_normal)\n",
    "new_score=np.hstack((new_score,np.array(new_score_anomaly)))\n",
    "\n",
    "#FPR,TPR,閾値を算出\n",
    "fpr_old,tpr_old,thr_old=metrics.roc_curve(y_true,old_score)\n",
    "fpr_new,tpr_new,thr_new=metrics.roc_curve(y_true,new_score)\n",
    "\n",
    "#AUC\n",
    "auc_old=metrics.auc(fpr_old,tpr_old)\n",
    "auc_new=metrics.auc(fpr_new,tpr_new)\n",
    "\n",
    "#ROC曲線をプロット\n",
    "plt.figure()\n",
    "plt.plot(fpr_old,tpr_old,label=\"Old method (area=%.2f)\"%auc_old)\n",
    "plt.plot(fpr_new,tpr_new,label=\"New method (area=%.2f)\"%auc_new)\n",
    "plt.legend()\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
